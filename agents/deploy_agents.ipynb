{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy RAG Agents\n",
    "\n",
    "This notebook:\n",
    "1. Logs agents to MLflow\n",
    "2. Registers models in Unity Catalog\n",
    "3. Deploys as serving endpoints using `agents.deploy()`\n",
    "\n",
    "**Prerequisites:**\n",
    "- Run `01_parse_pdfs.ipynb` and `02_create_indexes.ipynb` first\n",
    "- Vector Search indexes must be ONLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mlflow>=3.1.3 databricks-agents databricks-langchain langgraph pyyaml\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Parameters - widgets allow both interactive + job execution\ndbutils.widgets.text(\"catalog\", \"your_catalog\", \"Unity Catalog\")\ndbutils.widgets.text(\"schema\", \"rag_agents\", \"Schema name\")\n\nCATALOG = dbutils.widgets.get(\"catalog\")\nSCHEMA = dbutils.widgets.get(\"schema\")\n\n# Fail fast in job context if not configured\ntry:\n    is_job = bool(spark.conf.get(\"spark.databricks.job.id\"))\nexcept Exception:\n    is_job = False\n\nif is_job and CATALOG == \"your_catalog\":\n    raise ValueError(\"catalog parameter required for job execution\")\n\n# Agent configurations: (config_file, model_name, endpoint_name)\nAGENTS = [\n    (\"config_agent_a.yml\", \"agent_a_rag_model\", \"agent-a-rag\"),\n    (\"config_agent_b.yml\", \"agent_b_rag_model\", \"agent-b-rag\"),\n    (\"config_agent_c.yml\", \"agent_c_rag_model\", \"agent-c-rag\"),\n]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "import mlflow\n",
    "from databricks import agents\n",
    "from databricks_langchain import ChatDatabricks, VectorSearchRetrieverTool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import yaml\nfrom pathlib import Path\n\nimport mlflow\nfrom databricks import agents\nfrom databricks_langchain import ChatDatabricks, VectorSearchRetrieverTool\nfrom langgraph.prebuilt import create_react_agent\nfrom mlflow.models import infer_signature\n\nmlflow.set_registry_uri(\"databricks-uc\")\n\n\ndef load_config(config_name: str, catalog: str, schema: str) -> dict:\n    \"\"\"Load agent config from file and inject catalog/schema.\"\"\"\n    # Adjust path based on notebook location\n    config_path = Path(\"../agents\") / config_name\n    if not config_path.exists():\n        config_path = Path(\"/Workspace/Repos/\") / \"agents\" / config_name\n\n    with open(config_path) as f:\n        config = yaml.safe_load(f)\n\n    # Replace catalog.rag_agents prefix with actual values in vector_search_index\n    if \"vector_search_index\" in config:\n        parts = config[\"vector_search_index\"].split(\".\")\n        if len(parts) >= 3:\n            config[\"vector_search_index\"] = f\"{catalog}.{schema}.{parts[2]}\"\n\n    return config\n\n\ndef create_agent_from_config(config: dict):\n    \"\"\"Create RAG agent from configuration.\"\"\"\n    retriever_tool = VectorSearchRetrieverTool(\n        index_name=config[\"vector_search_index\"],\n        num_results=5,\n        columns=[\"content\", \"source\", \"chunk_id\"],\n        filters={},\n        text_column=\"content\",\n        tool_name=\"search_documents\",\n        tool_description=f\"Search the {config['agent_name']} knowledge base.\",\n    )\n\n    llm = ChatDatabricks(\n        endpoint=config.get(\"llm_endpoint\", \"databricks-meta-llama-3-3-70b-instruct\"),\n        temperature=0.1,\n        max_tokens=1024,\n    )\n\n    agent = create_react_agent(\n        model=llm,\n        tools=[retriever_tool],\n        state_modifier=config.get(\"system_prompt\", \"You are a helpful assistant.\"),\n    )\n\n    return agent"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log and Register Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def log_and_register_agent(config_file: str, model_name: str) -> str:\n    \"\"\"Log agent to MLflow and register in Unity Catalog.\"\"\"\n    config = load_config(config_file, CATALOG, SCHEMA)\n    agent = create_agent_from_config(config)\n\n    model_full_name = f\"{CATALOG}.{SCHEMA}.{model_name}\"\n    print(f\"Logging agent: {config['agent_name']} -> {model_full_name}\")\n\n    # Define input/output schema\n    input_example = {\"messages\": [{\"role\": \"user\", \"content\": \"Hello, how can you help?\"}]}\n\n    with mlflow.start_run(run_name=f\"deploy_{model_name}\"):\n        # Log the agent\n        model_info = mlflow.langchain.log_model(\n            lc_model=agent,\n            artifact_path=\"agent\",\n            input_example=input_example,\n            registered_model_name=model_full_name,\n        )\n\n    print(f\"  Registered: {model_full_name}\")\n    return model_full_name"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log all agents\n",
    "registered_models = {}\n",
    "for config_file, model_name, _ in AGENTS:\n",
    "    try:\n",
    "        full_name = log_and_register_agent(config_file, model_name)\n",
    "        registered_models[model_name] = full_name\n",
    "    except Exception as e:\n",
    "        print(f\"Error registering {model_name}: {e}\")\n",
    "        registered_models[model_name] = f\"Error: {e}\"\n",
    "\n",
    "print(\"\\n=== Registered Models ===\")\n",
    "for name, result in registered_models.items():\n",
    "    print(f\"{name}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Serving Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from databricks.sdk import WorkspaceClient\n\nw = WorkspaceClient()\n\n\ndef deploy_agent_endpoint(model_name: str, endpoint_name: str):\n    \"\"\"Deploy agent as serving endpoint using agents.deploy() (idempotent).\"\"\"\n    model_full_name = f\"{CATALOG}.{SCHEMA}.{model_name}\"\n\n    print(f\"Deploying: {model_full_name} -> {endpoint_name}\")\n\n    # Check if endpoint already exists\n    try:\n        existing = w.serving_endpoints.get(endpoint_name)\n        state = existing.state.ready if existing.state else \"UNKNOWN\"\n        print(f\"  Endpoint exists. Status: {state}\")\n        # agents.deploy will update existing endpoint with new model version\n    except Exception:\n        pass  # Endpoint doesn't exist, will be created\n\n    # Deploy using databricks-agents (handles create or update)\n    deployment = agents.deploy(\n        model_name=model_full_name,\n        model_version=1,  # Use latest; adjust as needed\n        endpoint_name=endpoint_name,\n        scale_to_zero=True,  # Cost optimization\n    )\n\n    print(f\"  Endpoint: {endpoint_name}\")\n    print(f\"  Status: {deployment.get('status', 'PENDING')}\")\n    return deployment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy all agents\n",
    "deployments = {}\n",
    "for config_file, model_name, endpoint_name in AGENTS:\n",
    "    if model_name in registered_models and not registered_models[model_name].startswith(\n",
    "        \"Error\"\n",
    "    ):\n",
    "        try:\n",
    "            deployment = deploy_agent_endpoint(model_name, endpoint_name)\n",
    "            deployments[endpoint_name] = \"deployed\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error deploying {endpoint_name}: {e}\")\n",
    "            deployments[endpoint_name] = f\"Error: {e}\"\n",
    "    else:\n",
    "        deployments[endpoint_name] = \"skipped (model not registered)\"\n",
    "\n",
    "print(\"\\n=== Deployments ===\")\n",
    "for endpoint, status in deployments.items():\n",
    "    print(f\"{endpoint}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def check_endpoint_status(endpoint_name: str):\n    \"\"\"Check serving endpoint status.\"\"\"\n    try:\n        endpoint = w.serving_endpoints.get(endpoint_name)\n        state = endpoint.state.ready if endpoint.state else \"UNKNOWN\"\n        print(f\"{endpoint_name}: {state}\")\n        return state\n    except Exception as e:\n        print(f\"{endpoint_name}: Error - {e}\")\n        return \"ERROR\"\n\n\nprint(\"=== Endpoint Status ===\")\nfor _, _, endpoint_name in AGENTS:\n    check_endpoint_status(endpoint_name)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test first endpoint\n",
    "test_endpoint = AGENTS[0][2]\n",
    "test_message = \"What information can you help me with?\"\n",
    "\n",
    "try:\n",
    "    client = w.serving_endpoints.get_open_ai_client()\n",
    "    response = client.chat.completions.create(\n",
    "        model=test_endpoint,\n",
    "        messages=[{\"role\": \"user\", \"content\": test_message}],\n",
    "        max_tokens=256,\n",
    "    )\n",
    "    print(f\"Endpoint: {test_endpoint}\")\n",
    "    print(f\"Query: {test_message}\")\n",
    "    print(f\"Response: {response.choices[0].message.content}\")\n",
    "except Exception as e:\n",
    "    print(f\"Test failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}