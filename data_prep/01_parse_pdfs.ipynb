{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse PDFs to Delta Tables\n",
    "\n",
    "This notebook:\n",
    "\n",
    "1. Reads PDFs from Unity Catalog volumes\n",
    "2. Parses content using `ai_parse_document()`\n",
    "3. Chunks text for vector search\n",
    "4. Saves to Delta tables with Change Data Feed enabled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install databricks-sdk\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters - widgets allow both interactive + job execution\n",
    "dbutils.widgets.text(\"catalog\", \"your_catalog\", \"Unity Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"rag_agents\", \"Schema name\")\n",
    "\n",
    "CATALOG = dbutils.widgets.get(\"catalog\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "# Fail fast in job context if not configured\n",
    "if spark.conf.get(\"spark.databricks.job.id\", None) and CATALOG == \"your_catalog\":\n",
    "    raise ValueError(\"catalog parameter required for job execution\")\n",
    "\n",
    "# Agent configurations: (volume_name, table_name)\n",
    "AGENTS = [\n",
    "    (\"vw_cars_wikipedia\", \"agent_a_docs\"),\n",
    "    (\"mercedes_cars_wikipedia\", \"agent_b_docs\"),\n",
    "    (\"bmw_cars_wikipedia\", \"agent_c_docs\"),\n",
    "]\n",
    "\n",
    "# Chunk settings\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType, StructField, StructType\n",
    "\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    if not text or len(text) <= chunk_size:\n",
    "        return [text] if text else []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Register UDF\n",
    "chunk_udf = F.udf(chunk_text, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_agent_pdfs(volume_name: str, table_name: str):\n",
    "    \"\"\"Process PDFs from volume and save to Delta table.\"\"\"\n",
    "\n",
    "    volume_path = f\"/Volumes/{CATALOG}/{SCHEMA}/{volume_name}\"\n",
    "    table_full_name = f\"{CATALOG}.{SCHEMA}.{table_name}\"\n",
    "\n",
    "    print(f\"Processing: {volume_path} -> {table_full_name}\")\n",
    "\n",
    "    # Ensure volume exists (idempotent)\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {CATALOG}.{SCHEMA}.{volume_name}\")\n",
    "\n",
    "    # Read PDFs using READ_FILES\n",
    "    pdf_df = spark.read.format(\"binaryFile\").load(f\"{volume_path}/*.pdf\")\n",
    "\n",
    "    # Parse documents using ai_parse_document\n",
    "    parsed_df = pdf_df.selectExpr(\n",
    "        \"path as source_path\",\n",
    "        \"ai_parse_document(content, 'text') as parsed_content\",\n",
    "    )\n",
    "\n",
    "    # Extract text and chunk\n",
    "    chunked_df = (\n",
    "        parsed_df.withColumn(\"full_text\", F.col(\"parsed_content.text\"))\n",
    "        .withColumn(\"chunks\", chunk_udf(F.col(\"full_text\")))\n",
    "        .select(\n",
    "            F.col(\"source_path\").alias(\"source\"),\n",
    "            F.posexplode(\"chunks\").alias(\"chunk_id\", \"content\"),\n",
    "        )\n",
    "        .withColumn(\"doc_id\", F.md5(F.concat(F.col(\"source\"), F.col(\"chunk_id\"))))\n",
    "    )\n",
    "\n",
    "    # Write to Delta with Change Data Feed (required for Delta Sync index)\n",
    "    chunked_df.write.format(\"delta\").mode(\"overwrite\").option(\n",
    "        \"delta.enableChangeDataFeed\", \"true\"\n",
    "    ).saveAsTable(table_full_name)\n",
    "\n",
    "    # Verify\n",
    "    count = spark.table(table_full_name).count()\n",
    "    print(f\"  Written {count} chunks to {table_full_name}\")\n",
    "\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all agents\n",
    "results = {}\n",
    "for volume_name, table_name in AGENTS:\n",
    "    try:\n",
    "        count = process_agent_pdfs(volume_name, table_name)\n",
    "        results[table_name] = count\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {volume_name}: {e}\")\n",
    "        results[table_name] = f\"Error: {e}\"\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "for table, result in results.items():\n",
    "    print(f\"{table}: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample from first table\n",
    "first_table = f\"{CATALOG}.{SCHEMA}.{AGENTS[0][1]}\"\n",
    "display(spark.table(first_table).limit(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
